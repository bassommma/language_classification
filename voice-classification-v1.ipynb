{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"code","source":"!pip install wandb -qU","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:00:41.679250Z","iopub.execute_input":"2025-05-23T01:00:41.679549Z","iopub.status.idle":"2025-05-23T01:00:58.358589Z","shell.execute_reply.started":"2025-05-23T01:00:41.679529Z","shell.execute_reply":"2025-05-23T01:00:58.357877Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip uninstall -y datasets\n\n# Install the latest stable version of datasets\n!pip install --upgrade datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:00:58.359945Z","iopub.execute_input":"2025-05-23T01:00:58.360223Z","iopub.status.idle":"2025-05-23T01:01:03.714415Z","shell.execute_reply.started":"2025-05-23T01:00:58.360198Z","shell.execute_reply":"2025-05-23T01:01:03.713679Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: datasets 3.6.0\nUninstalling datasets-3.6.0:\n  Successfully uninstalled datasets-3.6.0\nCollecting datasets\n  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, datasets\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:01:03.715450Z","iopub.execute_input":"2025-05-23T01:01:03.715715Z","iopub.status.idle":"2025-05-23T01:01:07.069864Z","shell.execute_reply.started":"2025-05-23T01:01:03.715690Z","shell.execute_reply":"2025-05-23T01:01:07.068952Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# for lang, dataset in mydatasets.items():\n#         # Create language directory\n        \n#         print(f\"Saving {lang} samples...\")\n#         for i, item in enumerate(dataset):\n#             # Get the audio data\n#             audio_array = np.array(item['audio']['array'])\n#             sampling_rate = item['audio']['sampling_rate']\n#             audio_tensor = torch.tensor(audio_array).unsqueeze(0)\n#             print(audio_tensor)\n#             break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:01:07.072102Z","iopub.execute_input":"2025-05-23T01:01:07.072319Z","iopub.status.idle":"2025-05-23T01:01:07.076346Z","shell.execute_reply.started":"2025-05-23T01:01:07.072299Z","shell.execute_reply":"2025-05-23T01:01:07.075529Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\nfrom datasets import Dataset, load_dataset\nimport os\nimport torch\nimport torch.nn as nn\nimport torchaudio\nimport pandas as pd\nimport librosa\nimport numpy as np\nfrom transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForSequenceClassification,Wav2Vec2Config,TrainingArguments,Trainer\nimport wandb\nimport random\nimport math\nimport json\ndef download_common_voice_subset(languages, num_samples=None, split=\"train\"):\n    \"\"\"\n    Download a subset of Common Voice for specified languages.\n    \n    Args:\n        languages: List of language codes\n        num_samples: Number of samples per language\n        split: Dataset split ('train', 'test', 'validation')\n    \n    Returns:\n        Dictionary with datasets for each language\n    \"\"\"\n    mydatasets = {}\n    \n    \n    for lang in languages:\n        print(f\"Loading {lang} dataset...\")\n        # Load only the specified split to save memory\n        dataset = load_dataset(\"google/fleurs\", lang, split=split,trust_remote_code=True)  # ~175MB\n        \n        \n        # Take only a subset\n        subset = dataset.select(range(min(num_samples, len(dataset))))\n        mydatasets[lang] = subset\n        \n        print(f\"Downloaded {len(subset)} samples for {lang}\")\n        \n    return mydatasets\n\ndef save_dataset_locally(mydatasets, output_dir=\"./language_samples\"):\n    \"\"\"Save audio files and metadata locally\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    metadata = []\n    \n    for lang, dataset in mydatasets.items():\n        # Create language directory\n        lang_dir = os.path.join(output_dir, lang)\n        os.makedirs(lang_dir, exist_ok=True)\n        \n        print(f\"Saving {lang} samples...\")\n        for i, item in enumerate(dataset):\n            # Get the audio data\n            audio_array = np.array(item['audio']['array'])\n            sampling_rate = item['audio']['sampling_rate']\n            audio_tensor = torch.tensor(audio_array).unsqueeze(0)\n            # Create a unique filename\n            filename = f\"{lang}_{i}.wav\"\n            output_path = os.path.join(lang_dir, filename)\n            \n            # Copy the audio file\n            torchaudio.save(output_path, audio_tensor, sampling_rate)\n            \n            # Add to metadata\n            metadata.append({\n                'filename': output_path,\n                'language': lang,\n                'text': item['transcription']\n            })\n            \n            if (i+1) % 10 == 0:\n                print(f\"Saved {i+1}/{len(dataset)} {lang} samples\")\n    \n    # Save metadata as CSV\n    df = pd.DataFrame(metadata)\n    df.to_csv(os.path.join(output_dir, \"metadata.csv\"), index=False)\n    print(f\"Saved metadata to {os.path.join(output_dir, 'metadata.csv')}\")\n    \n    return os.path.join(output_dir, \"metadata.csv\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:01:07.077166Z","iopub.execute_input":"2025-05-23T01:01:07.077422Z","iopub.status.idle":"2025-05-23T01:01:34.596977Z","shell.execute_reply.started":"2025-05-23T01:01:07.077385Z","shell.execute_reply":"2025-05-23T01:01:34.596166Z"}},"outputs":[{"name":"stderr","text":"2025-05-23 01:01:20.443547: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747962080.633250      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747962080.685457      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Usage\nlanguages=[\"en_us\",\"ar_eg\",\"fr_fr\"]\nmydatasets = download_common_voice_subset(languages, num_samples=500)\nmetadata_path = save_dataset_locally(mydatasets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:01:34.597803Z","iopub.execute_input":"2025-05-23T01:01:34.598376Z","iopub.status.idle":"2025-05-23T01:05:20.630061Z","shell.execute_reply.started":"2025-05-23T01:01:34.598351Z","shell.execute_reply":"2025-05-23T01:05:20.629461Z"}},"outputs":[{"name":"stdout","text":"Loading en_us dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/13.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"842602e2e72c476a82e23e16c304e0e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"fleurs.py:   0%|          | 0.00/12.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d044ccc8abe4e79bc01a62d76798fd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.tar.gz:   0%|          | 0.00/1.38G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fa8f7dc89df48918edcf37a28f419a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dev.tar.gz:   0%|          | 0.00/171M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35cebc24c2424dddacf0221a5add45ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.tar.gz:   0%|          | 0.00/290M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cace4439fc3441ddb8d9ae094d9b70f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.tsv:   0%|          | 0.00/1.41M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c79cde9299f043cf8da1bcd6eb0d9c29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dev.tsv:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8184ff988090467b8fa4092c4f4e4fe0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.tsv:   0%|          | 0.00/368k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bebe8ab3aa6a45c88091f7179629e2ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e8f79f80fbd447fb42e3a948909eade"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16a1720e543546549d8a4bd3ee3fbc8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6aa94b7d37ef4ec0b8d305c6838661a2"}},"metadata":{}},{"name":"stdout","text":"Downloaded 500 samples for en_us\nLoading ar_eg dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"train.tar.gz:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f71270e181174763b318af55a2e72256"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dev.tar.gz:   0%|          | 0.00/163M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbed3a7a79ee473e99e0774419f1795f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.tar.gz:   0%|          | 0.00/244M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83b71d1e047843d186bb4e500e6817b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.tsv:   0%|          | 0.00/1.53M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9029011dc0fc4c05b52385ab15ee2567"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dev.tsv:   0%|          | 0.00/215k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfff4523187f4f7baee2b61a3e227580"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.tsv:   0%|          | 0.00/324k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26377a309217430e89fcecfd3024d018"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a3fbced1a634a77b2a0fbc7c02907a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33e8b16503b7485588562ec2e007fbb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"982abd3027f540b891dc721a1ab20acd"}},"metadata":{}},{"name":"stdout","text":"Downloaded 500 samples for ar_eg\nLoading fr_fr dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"train.tar.gz:   0%|          | 0.00/1.73G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e047749f997453393f8c94d10a48eff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dev.tar.gz:   0%|          | 0.00/143M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45cc1ddc9bad4da6aa6f657e345d75b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.tar.gz:   0%|          | 0.00/349M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39d0e02c8fe94cd2828430e6272c34ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.tsv:   0%|          | 0.00/2.06M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e32f6d7135dc4fe7b6ce971f0cfd56cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dev.tsv:   0%|          | 0.00/181k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d80d3384c65487899c24ef045a8ab71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.tsv:   0%|          | 0.00/457k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f35b40a9d564494b998bacf9d5c00cb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ab1852fa8994e648ffd37b907418c8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07b22ed2251b4388a8a403bcbc4ac3a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f6b223e2574444dbeeab08a88b10179"}},"metadata":{}},{"name":"stdout","text":"Downloaded 500 samples for fr_fr\nSaving en_us samples...\nSaved 10/500 en_us samples\nSaved 20/500 en_us samples\nSaved 30/500 en_us samples\nSaved 40/500 en_us samples\nSaved 50/500 en_us samples\nSaved 60/500 en_us samples\nSaved 70/500 en_us samples\nSaved 80/500 en_us samples\nSaved 90/500 en_us samples\nSaved 100/500 en_us samples\nSaved 110/500 en_us samples\nSaved 120/500 en_us samples\nSaved 130/500 en_us samples\nSaved 140/500 en_us samples\nSaved 150/500 en_us samples\nSaved 160/500 en_us samples\nSaved 170/500 en_us samples\nSaved 180/500 en_us samples\nSaved 190/500 en_us samples\nSaved 200/500 en_us samples\nSaved 210/500 en_us samples\nSaved 220/500 en_us samples\nSaved 230/500 en_us samples\nSaved 240/500 en_us samples\nSaved 250/500 en_us samples\nSaved 260/500 en_us samples\nSaved 270/500 en_us samples\nSaved 280/500 en_us samples\nSaved 290/500 en_us samples\nSaved 300/500 en_us samples\nSaved 310/500 en_us samples\nSaved 320/500 en_us samples\nSaved 330/500 en_us samples\nSaved 340/500 en_us samples\nSaved 350/500 en_us samples\nSaved 360/500 en_us samples\nSaved 370/500 en_us samples\nSaved 380/500 en_us samples\nSaved 390/500 en_us samples\nSaved 400/500 en_us samples\nSaved 410/500 en_us samples\nSaved 420/500 en_us samples\nSaved 430/500 en_us samples\nSaved 440/500 en_us samples\nSaved 450/500 en_us samples\nSaved 460/500 en_us samples\nSaved 470/500 en_us samples\nSaved 480/500 en_us samples\nSaved 490/500 en_us samples\nSaved 500/500 en_us samples\nSaving ar_eg samples...\nSaved 10/500 ar_eg samples\nSaved 20/500 ar_eg samples\nSaved 30/500 ar_eg samples\nSaved 40/500 ar_eg samples\nSaved 50/500 ar_eg samples\nSaved 60/500 ar_eg samples\nSaved 70/500 ar_eg samples\nSaved 80/500 ar_eg samples\nSaved 90/500 ar_eg samples\nSaved 100/500 ar_eg samples\nSaved 110/500 ar_eg samples\nSaved 120/500 ar_eg samples\nSaved 130/500 ar_eg samples\nSaved 140/500 ar_eg samples\nSaved 150/500 ar_eg samples\nSaved 160/500 ar_eg samples\nSaved 170/500 ar_eg samples\nSaved 180/500 ar_eg samples\nSaved 190/500 ar_eg samples\nSaved 200/500 ar_eg samples\nSaved 210/500 ar_eg samples\nSaved 220/500 ar_eg samples\nSaved 230/500 ar_eg samples\nSaved 240/500 ar_eg samples\nSaved 250/500 ar_eg samples\nSaved 260/500 ar_eg samples\nSaved 270/500 ar_eg samples\nSaved 280/500 ar_eg samples\nSaved 290/500 ar_eg samples\nSaved 300/500 ar_eg samples\nSaved 310/500 ar_eg samples\nSaved 320/500 ar_eg samples\nSaved 330/500 ar_eg samples\nSaved 340/500 ar_eg samples\nSaved 350/500 ar_eg samples\nSaved 360/500 ar_eg samples\nSaved 370/500 ar_eg samples\nSaved 380/500 ar_eg samples\nSaved 390/500 ar_eg samples\nSaved 400/500 ar_eg samples\nSaved 410/500 ar_eg samples\nSaved 420/500 ar_eg samples\nSaved 430/500 ar_eg samples\nSaved 440/500 ar_eg samples\nSaved 450/500 ar_eg samples\nSaved 460/500 ar_eg samples\nSaved 470/500 ar_eg samples\nSaved 480/500 ar_eg samples\nSaved 490/500 ar_eg samples\nSaved 500/500 ar_eg samples\nSaving fr_fr samples...\nSaved 10/500 fr_fr samples\nSaved 20/500 fr_fr samples\nSaved 30/500 fr_fr samples\nSaved 40/500 fr_fr samples\nSaved 50/500 fr_fr samples\nSaved 60/500 fr_fr samples\nSaved 70/500 fr_fr samples\nSaved 80/500 fr_fr samples\nSaved 90/500 fr_fr samples\nSaved 100/500 fr_fr samples\nSaved 110/500 fr_fr samples\nSaved 120/500 fr_fr samples\nSaved 130/500 fr_fr samples\nSaved 140/500 fr_fr samples\nSaved 150/500 fr_fr samples\nSaved 160/500 fr_fr samples\nSaved 170/500 fr_fr samples\nSaved 180/500 fr_fr samples\nSaved 190/500 fr_fr samples\nSaved 200/500 fr_fr samples\nSaved 210/500 fr_fr samples\nSaved 220/500 fr_fr samples\nSaved 230/500 fr_fr samples\nSaved 240/500 fr_fr samples\nSaved 250/500 fr_fr samples\nSaved 260/500 fr_fr samples\nSaved 270/500 fr_fr samples\nSaved 280/500 fr_fr samples\nSaved 290/500 fr_fr samples\nSaved 300/500 fr_fr samples\nSaved 310/500 fr_fr samples\nSaved 320/500 fr_fr samples\nSaved 330/500 fr_fr samples\nSaved 340/500 fr_fr samples\nSaved 350/500 fr_fr samples\nSaved 360/500 fr_fr samples\nSaved 370/500 fr_fr samples\nSaved 380/500 fr_fr samples\nSaved 390/500 fr_fr samples\nSaved 400/500 fr_fr samples\nSaved 410/500 fr_fr samples\nSaved 420/500 fr_fr samples\nSaved 430/500 fr_fr samples\nSaved 440/500 fr_fr samples\nSaved 450/500 fr_fr samples\nSaved 460/500 fr_fr samples\nSaved 470/500 fr_fr samples\nSaved 480/500 fr_fr samples\nSaved 490/500 fr_fr samples\nSaved 500/500 fr_fr samples\nSaved metadata to ./language_samples/metadata.csv\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"mydatasets[\"ar_eg\"][0][\"audio\"][\"array\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:05:20.630772Z","iopub.execute_input":"2025-05-23T01:05:20.630999Z","iopub.status.idle":"2025-05-23T01:05:20.638591Z","shell.execute_reply.started":"2025-05-23T01:05:20.630981Z","shell.execute_reply":"2025-05-23T01:05:20.637878Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"array([0.        , 0.        , 0.        , ..., 0.00022024, 0.00048441,\n       0.0006727 ])"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"df = pd.read_csv(metadata_path, encoding='utf-8')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:05:20.639261Z","iopub.execute_input":"2025-05-23T01:05:20.639520Z","iopub.status.idle":"2025-05-23T01:05:20.660699Z","shell.execute_reply.started":"2025-05-23T01:05:20.639494Z","shell.execute_reply":"2025-05-23T01:05:20.660120Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"for i, row in df.iterrows():\n    print(f\"{row}\")\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:05:20.661469Z","iopub.execute_input":"2025-05-23T01:05:20.661686Z","iopub.status.idle":"2025-05-23T01:05:20.666400Z","shell.execute_reply.started":"2025-05-23T01:05:20.661671Z","shell.execute_reply":"2025-05-23T01:05:20.665856Z"}},"outputs":[{"name":"stdout","text":"filename                 ./language_samples/en_us/en_us_0.wav\nlanguage                                                en_us\ntext        a tornado is a spinning column of very low-pre...\nName: 0, dtype: object\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"def prepare_dataset(metadata_csv):\n    \"\"\"Transform metadata CSV into a format ready for training\"\"\"\n    \n    # Load your metadata\n    df = pd.read_csv(metadata_csv, encoding='utf-8')\n    \n    # Create mapping from language to ID\n    languages = df['language'].unique()\n    language_to_id = {lang: i for i, lang in enumerate(languages)}\n    id_to_language = {i: lang for lang, i in language_to_id.items()}\n    \n    # Create dictionaries that will be used to create the dataset\n    paths = []\n    languages = []\n    language_ids = []\n    \n    # Process each file\n    for _, row in df.iterrows():\n        try:\n            # Store only the path - we'll load audio on-the-fly\n            paths.append(row['filename'])\n            languages.append(row['language'])\n            language_ids.append(language_to_id[row['language']])\n        except Exception as e:\n            print(f\"Error processing {row['filename']}: {e}\")\n    \n    # Create the dataset directly\n    dataset_dict = {\n        'path': paths,\n        'language': languages,\n        'language_id': language_ids\n    }\n    dataset = Dataset.from_dict(dataset_dict)\n    \n    # Add a function to load audio when needed\n    def load_audio(example):\n        audio, sr = librosa.load(example['path'], sr=16000)\n        \n        return {'audio': np.array(audio, dtype=np.float32), 'sampling_rate': 16000}\n    \n    # Apply the audio loading function\n    dataset = dataset.map(load_audio)\n    \n    return dataset, language_to_id, id_to_language","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:05:20.668839Z","iopub.execute_input":"2025-05-23T01:05:20.669236Z","iopub.status.idle":"2025-05-23T01:05:20.685840Z","shell.execute_reply.started":"2025-05-23T01:05:20.669220Z","shell.execute_reply":"2025-05-23T01:05:20.685197Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"dataset, language_to_id, id_to_language = prepare_dataset(\"/kaggle/working/language_samples/metadata.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:05:20.686425Z","iopub.execute_input":"2025-05-23T01:05:20.686655Z","iopub.status.idle":"2025-05-23T01:05:27.674934Z","shell.execute_reply.started":"2025-05-23T01:05:20.686635Z","shell.execute_reply":"2025-05-23T01:05:27.674162Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42e76dbdc7424cb690c4536dcf752b54"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# import numpy as np\n\n# # Make sure you have called prepare_dataset and assigned its output:\n# # dataset, language_to_id, id_to_language = prepare_dataset(metadata_path)\n\n# # --- Start of inspection code ---\n\n# # Check the first sample (assuming it's English based on your previous output)\n# en_sample = dataset[0]\n\n# print(f\"Keys in dataset[0]: {en_sample.keys()}\")\n\n# if 'audio' in en_sample:\n#     audio_data_en = en_sample['audio']\n    \n#     # If it's a list, convert to NumPy array. If it's already an array, this does nothing.\n#     if isinstance(audio_data_en, list):\n#         print(\"Warning: audio data for English sample is a list, converting to numpy array.\")\n#         audio_data_en = np.array(audio_data_en, dtype=np.float32) # Ensure correct dtype\n    \n#     print(f\"English audio sample (index 0):\")\n#     print(f\"Type: {type(audio_data_en)}\")\n#     print(f\"Shape: {audio_data_en.shape}\")\n#     print(f\"Mean: {np.mean(audio_data_en):.4f}\")\n#     print(f\"Std: {np.std(audio_data_en):.4f}\")\n#     print(f\"Language: {en_sample['language']}, ID: {en_sample['language_id']}\")\n    \n#     # You can also check if it's all zeros\n#     if np.all(audio_data_en == 0):\n#         print(\"ALERT: English audio sample is all zeros!\")\n# else:\n#     print(\"Error: 'audio' key not found in dataset[0]. Did map(load_audio) run correctly?\")\n\n\n# # Check an Arabic sample\n# # Find an Arabic sample safely, as order might vary\n# ar_sample_index = -1\n# for i, ex in enumerate(dataset):\n#     if ex['language'] == 'ar_eg':\n#         ar_sample_index = i\n#         break\n\n# if ar_sample_index != -1:\n#     ar_sample = dataset[ar_sample_index]\n    \n#     if 'audio' in ar_sample:\n#         audio_data_ar = ar_sample['audio']\n        \n#         # If it's a list, convert to NumPy array\n#         if isinstance(audio_data_ar, list):\n#             print(f\"\\nWarning: audio data for Arabic sample (index {ar_sample_index}) is a list, converting to numpy array.\")\n#             audio_data_ar = np.array(audio_data_ar, dtype=np.float32) # Ensure correct dtype\n        \n#         print(f\"\\nArabic audio sample (index {ar_sample_index}):\")\n#         print(f\"Type: {type(audio_data_ar)}\")\n#         print(f\"Shape: {audio_data_ar.shape}\")\n#         print(f\"Mean: {np.mean(audio_data_ar):.4f}\")\n#         print(f\"Std: {np.std(audio_data_ar):.4f}\")\n#         print(f\"Language: {ar_sample['language']}, ID: {ar_sample['language_id']}\")\n\n#         # You can also check if it's all zeros\n#         if np.all(audio_data_ar == 0):\n#             print(\"ALERT: Arabic audio sample is all zeros!\")\n#     else:\n#         print(f\"\\nError: 'audio' key not found in dataset[{ar_sample_index}]. Did map(load_audio) run correctly?\")\n# else:\n#     print(\"\\nCould not find an Arabic sample in the dataset. Please check your dataset composition.\")\n\n# # --- End of inspection code ---","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:05:27.675822Z","iopub.execute_input":"2025-05-23T01:05:27.676350Z","iopub.status.idle":"2025-05-23T01:05:27.681010Z","shell.execute_reply.started":"2025-05-23T01:05:27.676325Z","shell.execute_reply":"2025-05-23T01:05:27.680283Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb_key\")\nwandb.login(key = secret_value_0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:05:27.681812Z","iopub.execute_input":"2025-05-23T01:05:27.682098Z","iopub.status.idle":"2025-05-23T01:05:34.656934Z","shell.execute_reply.started":"2025-05-23T01:05:27.682073Z","shell.execute_reply":"2025-05-23T01:05:34.656367Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbasem-yasser\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n\ndef prepare_features(examples):\n    \"\"\"Convert audio to input features\"\"\"\n    \n    audio_inputs = []\n    for audio_item in examples[\"audio\"]:\n        if isinstance(audio_item, list):\n            audio_inputs.append(np.array(audio_item, dtype=np.float32))\n        else:\n            audio_inputs.append(audio_item)\n            \n    inputs = feature_extractor(\n        audio_inputs,  \n        sampling_rate=feature_extractor.sampling_rate,\n        return_attention_mask=True,\n        padding=\"longest\",\n        truncation=True,\n        max_length=16000 * 5,\n        return_tensors=\"pt\"\n    )\n    \n    # Ensure we're returning NumPy arrays with explicit dtypes\n    # This is the key: converting, if needed, and explicitly setting dtypes\n    input_values = np.asarray(inputs['input_values'], dtype=np.float32)\n    attention_mask = np.asarray(inputs['attention_mask'], dtype=np.int8)\n    labels = np.asarray(examples[\"language_id\"], dtype=np.int64)\n    \n    # Return the dictionary with explicitly typed NumPy arrays\n    return {\n        'input_values': input_values,\n        'attention_mask': attention_mask,\n        'labels': labels\n    }\n\n# Assuming 'dataset' is your Hugging Face Dataset object from prepare_dataset()\n# We'll apply the map function and then EXPLICITLY set the format to 'numpy'\n\nall_dataset = dataset.map(\n    prepare_features,\n    batched=True,\n    # num_proc=os.cpu_count(),\n    remove_columns=['audio', 'sampling_rate', 'language', 'path'],\n    load_from_cache_file=False\n)\n\n# This is the key addition: explicitly set the format to 'numpy'\n# This ensures the dataset stores arrays as NumPy arrays, not lists\nall_dataset = all_dataset.with_format(\"numpy\")\n\n# Now, let's check the type again\nprint(\"\\nProcessed Dataset Example (first item):\")\nprint(all_dataset[0].keys())\nprint(f\"Input values type: {type(all_dataset[0]['input_values'])}\")\nprint(f\"Input values shape: {all_dataset[0]['input_values'].shape}\")\nprint(f\"Attention mask type: {type(all_dataset[0]['attention_mask'])}\")\nprint(f\"Attention mask shape: {all_dataset[0]['attention_mask'].shape}\")\nprint(f\"Label type: {type(all_dataset[0]['labels'])}\")\nprint(f\"Label: {all_dataset[0]['labels']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:05:34.657568Z","iopub.execute_input":"2025-05-23T01:05:34.658093Z","iopub.status.idle":"2025-05-23T01:07:39.369917Z","shell.execute_reply.started":"2025-05-23T01:05:34.658074Z","shell.execute_reply":"2025-05-23T01:07:39.369284Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec032608eb854335b71c509e812a1ec7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"453d358550ae4afcb5cc9aa753e1b09f"}},"metadata":{}},{"name":"stdout","text":"\nProcessed Dataset Example (first item):\ndict_keys(['language_id', 'input_values', 'attention_mask', 'labels'])\nInput values type: <class 'numpy.ndarray'>\nInput values shape: (80000,)\nAttention mask type: <class 'numpy.ndarray'>\nAttention mask shape: (80000,)\nLabel type: <class 'numpy.int64'>\nLabel: 0\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# df = pd.read_csv(\"./language_samples/metadata.csv\", encoding=\"utf-8\")\n# arabic_samples = df[df['language'] == 'ar_eg']\n# print(arabic_samples['text'].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:07:39.370640Z","iopub.execute_input":"2025-05-23T01:07:39.370907Z","iopub.status.idle":"2025-05-23T01:07:39.373879Z","shell.execute_reply.started":"2025-05-23T01:07:39.370880Z","shell.execute_reply":"2025-05-23T01:07:39.373349Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# model_path=\"facebook/wav2vec2-base-960h\"\n# accent_labels =[\"english\",\"arabic\"]\n# model = AccentClassifier(language_to_id,id_to_language)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:07:39.374705Z","iopub.execute_input":"2025-05-23T01:07:39.374963Z","iopub.status.idle":"2025-05-23T01:07:39.390849Z","shell.execute_reply.started":"2025-05-23T01:07:39.374941Z","shell.execute_reply":"2025-05-23T01:07:39.390309Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class AccentClassifier(Wav2Vec2ForSequenceClassification):\n    def __init__(self, pretrained_model_name, language_to_id, id_to_language):\n        super().__init__(\n            Wav2Vec2Config.from_pretrained(\n                pretrained_model_name,\n                num_labels=len(language_to_id),\n                label2id=language_to_id,\n                id2label=id_to_language\n            )\n        )\n        # Load pretrained weights\n        self.load_state_dict(Wav2Vec2ForSequenceClassification.from_pretrained(pretrained_model_name).state_dict())\n        # Initialize feature extractor\n        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(pretrained_model_name)\n        # Store language mappings\n        self.language_to_id = language_to_id\n        self.id_to_language = id_to_language\n    \n    def preprocess(self, audio_file):\n        # Load and resample audio\n        audio, sr = librosa.load(audio_file, sr=16000)\n        audio = np.asarray(audio, dtype=np.float32)\n        inputs = self.feature_extractor(audio, sampling_rate=16000, return_tensors=\"pt\")\n        return inputs\n        \n    def predict(self, audio_file):\n        self.eval()  # Ensure model is in evaluation mode\n        inputs = self.preprocess(audio_file)\n        # Model inference\n        with torch.no_grad():\n            outputs = self(**inputs).logits\n            predicted_class = torch.argmax(outputs, dim=1).item()\n        return self.id_to_language[predicted_class]\n    \n    def save_model(self, output_dir):\n        # Save model, feature extractor, and mappings\n        self.save_pretrained(output_dir)\n        self.feature_extractor.save_pretrained(output_dir)\n        \n        # Save language mappings\n        with open(f\"{output_dir}/language_mapping.json\", 'w') as f:\n            json.dump({\n                'language_to_id': self.language_to_id,\n                'id_to_language': self.id_to_language\n            }, f)\n    \n    @classmethod\n    def from_pretrained(cls, model_path):\n        # Load language mappings first\n        with open(f\"{model_path}/config.json\", 'r') as f:\n            mappings = json.load(f)\n        \n        # Convert id keys from strings back to integers if needed\n        id_to_language = {int(k): v for k, v in mappings['id2label'].items()}\n        \n        # Create a temporary basic model to load weights\n        is_checkpoint = \"checkpoint-\" in model_path\n        parent_dir = os.path.dirname(model_path) if is_checkpoint else model_path\n    \n    # Load model weights from checkpoint\n        temp_model = Wav2Vec2ForSequenceClassification.from_pretrained(model_path)\n    \n    # Load feature extractor from parent directory instead of checkpoint\n        feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(parent_dir)\n        \n        # Create new instance\n        instance = cls(\n            pretrained_model_name=\"facebook/wav2vec2-base-960h\", \n            language_to_id=mappings['label2id'], \n            id_to_language=id_to_language\n        )\n        \n        # Copy loaded weights\n        instance.load_state_dict(temp_model.state_dict())\n        instance.feature_extractor = feature_extractor\n        \n        return instance","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:07:39.391637Z","iopub.execute_input":"2025-05-23T01:07:39.391864Z","iopub.status.idle":"2025-05-23T01:07:39.412454Z","shell.execute_reply.started":"2025-05-23T01:07:39.391847Z","shell.execute_reply":"2025-05-23T01:07:39.411767Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"all_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:07:39.413236Z","iopub.execute_input":"2025-05-23T01:07:39.413557Z","iopub.status.idle":"2025-05-23T01:07:39.430090Z","shell.execute_reply.started":"2025-05-23T01:07:39.413534Z","shell.execute_reply":"2025-05-23T01:07:39.429540Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['language_id', 'input_values', 'attention_mask', 'labels'],\n    num_rows: 1500\n})"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"dataset = all_dataset.train_test_split(test_size=0.2, seed=42)  # 80% train, 20% validation\ntrain_dataset=dataset['train']\neval_dataset=dataset['test']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:07:39.430796Z","iopub.execute_input":"2025-05-23T01:07:39.431043Z","iopub.status.idle":"2025-05-23T01:07:39.545377Z","shell.execute_reply.started":"2025-05-23T01:07:39.431022Z","shell.execute_reply":"2025-05-23T01:07:39.544870Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"count,unique=np.unique(eval_dataset['labels'],return_counts=True)\ndic={x:y for x,y in zip(count,unique)}\ndic\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:07:39.546149Z","iopub.execute_input":"2025-05-23T01:07:39.546395Z","iopub.status.idle":"2025-05-23T01:07:39.557998Z","shell.execute_reply.started":"2025-05-23T01:07:39.546373Z","shell.execute_reply":"2025-05-23T01:07:39.557454Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"{0: 101, 1: 94, 2: 105}"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"count,unique=np.unique(train_dataset['labels'],return_counts=True)\ndic={x:y for x,y in zip(count,unique)}\ndic\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:07:39.558734Z","iopub.execute_input":"2025-05-23T01:07:39.559204Z","iopub.status.idle":"2025-05-23T01:07:39.571202Z","shell.execute_reply.started":"2025-05-23T01:07:39.559187Z","shell.execute_reply":"2025-05-23T01:07:39.570592Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"{0: 399, 1: 406, 2: 395}"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"import numpy as np\nlabels = train_dataset['labels']\nunique, counts = np.unique(labels, return_counts=True)\nprint(\"Label Distribution:\", dict(zip(unique, counts)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:07:39.571906Z","iopub.execute_input":"2025-05-23T01:07:39.572144Z","iopub.status.idle":"2025-05-23T01:07:39.591095Z","shell.execute_reply.started":"2025-05-23T01:07:39.572121Z","shell.execute_reply":"2025-05-23T01:07:39.590581Z"}},"outputs":[{"name":"stdout","text":"Label Distribution: {0: 399, 1: 406, 2: 395}\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# def collate_audio_classification(batch):\n#     \"\"\"Custom collator for Wav2Vec2 audio classification.\"\"\"\n#     input_features = [{\"input_values\": item[\"input_values\"]} for item in batch]\n#     labels = [item[\"labels\"] for item in batch]\n    \n#     # Pad the inputs\n#     padded_batch = feature_extractor.pad(\n#         input_features,\n#         padding=True,\n#         return_tensors=\"pt\",\n#     )\n    \n#     # Create the final batch dict\n#     batch_dict = {\n#         \"input_values\": padded_batch[\"input_values\"]\n#     }\n    \n#     # Add attention mask if present\n#     if \"attention_mask\" in padded_batch:\n#         batch_dict[\"attention_mask\"] = padded_batch[\"attention_mask\"]\n    \n#     # Add labels\n#     batch_dict[\"labels\"] = torch.tensor(labels, dtype=torch.long)\n    \n#     return batch_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:07:39.591722Z","iopub.execute_input":"2025-05-23T01:07:39.591898Z","iopub.status.idle":"2025-05-23T01:07:39.595266Z","shell.execute_reply.started":"2025-05-23T01:07:39.591886Z","shell.execute_reply":"2025-05-23T01:07:39.594635Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"from transformers import Wav2Vec2Config, Wav2Vec2ForSequenceClassification, TrainingArguments, Trainer\nimport evaluate\nimport numpy as np\nfrom transformers import EarlyStoppingCallback\n\n\n\n\n# 1. First prepare your model configuration\nconfig = Wav2Vec2Config.from_pretrained(\n    \"facebook/wav2vec2-base-960h\",\n    num_labels=len(language_to_id),  \n    label2id=language_to_id,         \n    id2label=id_to_language,        \n    finetuning_task=\"audio-classification\",\n    # classifier_dropout=0.1\n)\n\n# 2. Initialize model with this configuration\nmodel = Wav2Vec2ForSequenceClassification.from_pretrained(\n    \"facebook/wav2vec2-base-960h\",\n    config=config,\n    ignore_mismatched_sizes=True  # Helpful when loading pre-trained models\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:07:39.595989Z","iopub.execute_input":"2025-05-23T01:07:39.596298Z","iopub.status.idle":"2025-05-23T01:07:43.566541Z","shell.execute_reply.started":"2025-05-23T01:07:39.596278Z","shell.execute_reply":"2025-05-23T01:07:43.565934Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57cdade1ade241e5b2452925fb8e3208"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c46c3f1eb1942f48b94f92a8dfeb717"}},"metadata":{}},{"name":"stderr","text":"Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.masked_spec_embed']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:07:43.567172Z","iopub.execute_input":"2025-05-23T01:07:43.567375Z","iopub.status.idle":"2025-05-23T01:07:43.573304Z","shell.execute_reply.started":"2025-05-23T01:07:43.567358Z","shell.execute_reply":"2025-05-23T01:07:43.572605Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"Wav2Vec2ForSequenceClassification(\n  (wav2vec2): Wav2Vec2Model(\n    (feature_extractor): Wav2Vec2FeatureEncoder(\n      (conv_layers): ModuleList(\n        (0): Wav2Vec2GroupNormConvLayer(\n          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n          (activation): GELUActivation()\n          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n        )\n        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n          (activation): GELUActivation()\n        )\n        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n          (activation): GELUActivation()\n        )\n      )\n    )\n    (feature_projection): Wav2Vec2FeatureProjection(\n      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (projection): Linear(in_features=512, out_features=768, bias=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): Wav2Vec2Encoder(\n      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n        (conv): ParametrizedConv1d(\n          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n          (parametrizations): ModuleDict(\n            (weight): ParametrizationList(\n              (0): _WeightNorm()\n            )\n          )\n        )\n        (padding): Wav2Vec2SamePadLayer()\n        (activation): GELUActivation()\n      )\n      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (layers): ModuleList(\n        (0-11): 12 x Wav2Vec2EncoderLayer(\n          (attention): Wav2Vec2SdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (projector): Linear(in_features=768, out_features=256, bias=True)\n  (classifier): Linear(in_features=256, out_features=3, bias=True)\n)"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"# for param in model.wav2vec2.encoder.parameters():\n#     param.requires_grad =False\n\n# for param in model.classifier.parameters():  \n#     param.requires_grad =True\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:07:43.574125Z","iopub.execute_input":"2025-05-23T01:07:43.574531Z","iopub.status.idle":"2025-05-23T01:07:43.589536Z","shell.execute_reply.started":"2025-05-23T01:07:43.574513Z","shell.execute_reply":"2025-05-23T01:07:43.588832Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# num_layers_to_unfreeze=2\n# total_encoder_layers=len(model.wav2vec2.encoder.layers)\n# for i in range (total_encoder_layers-num_layers_to_unfreeze,total_encoder_layers):\n    \n#     for param in model.wav2vec2.encoder.layers[i].parameters():\n#         param.requires_grad =True\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:07:43.590171Z","iopub.execute_input":"2025-05-23T01:07:43.590390Z","iopub.status.idle":"2025-05-23T01:07:43.602897Z","shell.execute_reply.started":"2025-05-23T01:07:43.590374Z","shell.execute_reply":"2025-05-23T01:07:43.602318Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params:.2%} of total)\")\n\n# # Count parameters by component (optional but helpful)\n# classifier_params = sum(p.numel() for p in model.classifier.parameters())\n# print(f\"Classifier parameters: {classifier_params:,}\")\n\n# unfrozen_encoder_params = sum(\n#     p.numel() for layer_idx in range(total_encoder_layers - num_layers_to_unfreeze, total_encoder_layers)\n#     for p in model.wav2vec2.encoder.layers[layer_idx].parameters() if p.requires_grad\n# )\n# print(f\"Unfrozen encoder parameters: {unfrozen_encoder_params:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:07:43.606129Z","iopub.execute_input":"2025-05-23T01:07:43.606324Z","iopub.status.idle":"2025-05-23T01:07:43.618361Z","shell.execute_reply.started":"2025-05-23T01:07:43.606309Z","shell.execute_reply":"2025-05-23T01:07:43.617684Z"}},"outputs":[{"name":"stdout","text":"Trainable parameters: 94,569,347 (100.00% of total)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    \"\"\"Compute metrics for evaluation\"\"\"\n    predictions = eval_pred.predictions.argmax(-1)\n    labels = eval_pred.label_ids\n    \n    accuracy = metric.compute(predictions=predictions, references=labels)\n\n\n    class_names = list(language_to_id.keys())\n    cm = confusion_matrix(labels, predictions)\n    fig, ax = plt.subplots(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    \n    # Log to wandb\n    if \"wandb\" in training_args.report_to:\n        wandb.log({\"confusion_matrix\": wandb.Image(fig)})\n    \n    plt.close(fig)\n\n\n    \n    \n    # Add confusion matrix calculation\n    conf_mat = confusion_matrix(labels, predictions).tolist()\n    \n    # Calculate per-class metrics\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        labels, predictions, average=None\n    )\n    \n    # Get class names for reporting\n    class_names = [id_to_language[i] for i in range(len(id_to_language))]\n    \n    # Create per-class metrics dictionary\n    per_class_metrics = {}\n    for i, name in enumerate(class_names):\n        per_class_metrics[f\"{name}_precision\"] = precision[i]\n        per_class_metrics[f\"{name}_recall\"] = recall[i]\n        per_class_metrics[f\"{name}_f1\"] = f1[i]\n    \n    # Return all metrics\n    return {\n        \"accuracy\": accuracy[\"accuracy\"],\n        \"confusion_matrix\": conf_mat,\n        **per_class_metrics\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:07:43.619013Z","iopub.execute_input":"2025-05-23T01:07:43.619206Z","iopub.status.idle":"2025-05-23T01:07:43.639071Z","shell.execute_reply.started":"2025-05-23T01:07:43.619192Z","shell.execute_reply":"2025-05-23T01:07:43.638324Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n\n# If you want to visualize the confusion matrix (optional)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\n\n# 3. Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./lsatone\",\n    eval_strategy=\"epoch\",     # Evaluate during training\n    # eval_steps=100,                  # Evaluate every 100 steps\n    learning_rate= 3e-5  ,             \n    per_device_train_batch_size=32,   # Adjust based on your GPU memory\n    per_device_eval_batch_size=32,\n    num_train_epochs=6,            # Adjust based on your dataset size and performance\n    weight_decay=0.01,               # Add some regularization\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    push_to_hub=False,               # Set to True if you want to upload your model to the Hub\n    report_to=\"wandb\",               # Use wandb for logging\n    logging_dir='./logs',\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    lr_scheduler_type=\"linear\",\n    warmup_ratio=0.15,\n    max_grad_norm=1.0,\n    fp16=True,\n    gradient_accumulation_steps=2,  #calc gradiant and do change in waits every 2 steps\n    save_total_limit=2,             #save only 2 checkpoints\n  \n)\n\n# 4. Define compute_metrics function\nmetric = evaluate.load(\"accuracy\")  # Load accuracy metric\n\n\n\n\ntraining_callbacks = [\n    EarlyStoppingCallback(early_stopping_patience=5),\n    # Optional: Add learning rate monitoring\n    \n]\n\n\n# 7. Initialize Trainer WITHOUT a data_collator\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics,\n    # data_collator=collate_audio_classification,\n    callbacks=training_callbacks\n    \n)\n\n\ntrain_result = trainer.train()\ntrain_loss = train_result.training_loss\nprint(f\"Final Training Loss: {train_loss}\")\n# 9. Evaluate on the test set\nfinal_metrics = trainer.evaluate()\nprint(f\"Final evaluation metrics: {final_metrics}\")\n\n# 10. Save the model\nmodel_path = \"./results/wav2vec2-language-id/lasttt\"\nmodel.save_pretrained(model_path)\nfeature_extractor.save_pretrained(model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:07:43.639844Z","iopub.execute_input":"2025-05-23T01:07:43.640157Z","iopub.status.idle":"2025-05-23T01:12:36.221026Z","shell.execute_reply.started":"2025-05-23T01:07:43.640134Z","shell.execute_reply":"2025-05-23T01:12:36.220447Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da63a5f7826040eea23a81bb658a205e"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.11"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250523_010745-uxy1ct1z</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/basem-yasser/huggingface/runs/uxy1ct1z' target=\"_blank\">./lsatone</a></strong> to <a href='https://wandb.ai/basem-yasser/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/basem-yasser/huggingface' target=\"_blank\">https://wandb.ai/basem-yasser/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/basem-yasser/huggingface/runs/uxy1ct1z' target=\"_blank\">https://wandb.ai/basem-yasser/huggingface/runs/uxy1ct1z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='114' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [114/114 04:37, Epoch 6/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Confusion Matrix</th>\n      <th>En Us Precision</th>\n      <th>En Us Recall</th>\n      <th>En Us F1</th>\n      <th>Ar Eg Precision</th>\n      <th>Ar Eg Recall</th>\n      <th>Ar Eg F1</th>\n      <th>Fr Fr Precision</th>\n      <th>Fr Fr Recall</th>\n      <th>Fr Fr F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.097300</td>\n      <td>1.040391</td>\n      <td>0.543333</td>\n      <td>[[58, 0, 43], [0, 0, 94], [0, 0, 105]]</td>\n      <td>1.000000</td>\n      <td>0.574257</td>\n      <td>0.729560</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.433884</td>\n      <td>1.000000</td>\n      <td>0.605187</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.017200</td>\n      <td>0.768571</td>\n      <td>0.966667</td>\n      <td>[[100, 1, 0], [0, 86, 8], [1, 0, 104]]</td>\n      <td>0.990099</td>\n      <td>0.990099</td>\n      <td>0.990099</td>\n      <td>0.988506</td>\n      <td>0.914894</td>\n      <td>0.950276</td>\n      <td>0.928571</td>\n      <td>0.990476</td>\n      <td>0.958525</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.763800</td>\n      <td>0.486882</td>\n      <td>0.996667</td>\n      <td>[[100, 1, 0], [0, 94, 0], [0, 0, 105]]</td>\n      <td>1.000000</td>\n      <td>0.990099</td>\n      <td>0.995025</td>\n      <td>0.989474</td>\n      <td>1.000000</td>\n      <td>0.994709</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.519400</td>\n      <td>0.340210</td>\n      <td>0.993333</td>\n      <td>[[100, 1, 0], [0, 93, 1], [0, 0, 105]]</td>\n      <td>1.000000</td>\n      <td>0.990099</td>\n      <td>0.995025</td>\n      <td>0.989362</td>\n      <td>0.989362</td>\n      <td>0.989362</td>\n      <td>0.990566</td>\n      <td>1.000000</td>\n      <td>0.995261</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.359100</td>\n      <td>0.263152</td>\n      <td>0.996667</td>\n      <td>[[100, 1, 0], [0, 94, 0], [0, 0, 105]]</td>\n      <td>1.000000</td>\n      <td>0.990099</td>\n      <td>0.995025</td>\n      <td>0.989474</td>\n      <td>1.000000</td>\n      <td>0.994709</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.326200</td>\n      <td>0.240446</td>\n      <td>0.996667</td>\n      <td>[[100, 1, 0], [0, 94, 0], [0, 0, 105]]</td>\n      <td>1.000000</td>\n      <td>0.990099</td>\n      <td>0.995025</td>\n      <td>0.989474</td>\n      <td>1.000000</td>\n      <td>0.994709</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Final Training Loss: 0.6603567223799857\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 00:03]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Final evaluation metrics: {'eval_loss': 0.4868823289871216, 'eval_accuracy': 0.9966666666666667, 'eval_confusion_matrix': [[100, 1, 0], [0, 94, 0], [0, 0, 105]], 'eval_en_us_precision': 1.0, 'eval_en_us_recall': 0.9900990099009901, 'eval_en_us_f1': 0.9950248756218906, 'eval_ar_eg_precision': 0.9894736842105263, 'eval_ar_eg_recall': 1.0, 'eval_ar_eg_f1': 0.9947089947089947, 'eval_fr_fr_precision': 1.0, 'eval_fr_fr_recall': 1.0, 'eval_fr_fr_f1': 1.0, 'eval_runtime': 3.6829, 'eval_samples_per_second': 81.457, 'eval_steps_per_second': 2.715, 'epoch': 6.0}\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"['./results/wav2vec2-language-id/lasttt/preprocessor_config.json']"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"main=\"/kaggle/working/\"\nos.makedirs(main+'loaded_model'+'checkpoint',exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:12:36.221775Z","iopub.execute_input":"2025-05-23T01:12:36.222036Z","iopub.status.idle":"2025-05-23T01:12:36.226774Z","shell.execute_reply.started":"2025-05-23T01:12:36.222007Z","shell.execute_reply":"2025-05-23T01:12:36.226118Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"loaded_model = Wav2Vec2ForSequenceClassification.from_pretrained(\"/kaggle/working/results/wav2vec2-language-id/lasttt\")\n  \n# Load the feature extractor\nmyfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:12:36.227530Z","iopub.execute_input":"2025-05-23T01:12:36.227690Z","iopub.status.idle":"2025-05-23T01:12:36.462671Z","shell.execute_reply.started":"2025-05-23T01:12:36.227677Z","shell.execute_reply":"2025-05-23T01:12:36.461983Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"languages=[\"en_us\",\"ar_eg\",\"fr_fr\"]\nmydatasets_test = download_common_voice_subset(languages, num_samples=3,split=\"test\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:12:36.463371Z","iopub.execute_input":"2025-05-23T01:12:36.463607Z","iopub.status.idle":"2025-05-23T01:12:38.345176Z","shell.execute_reply.started":"2025-05-23T01:12:36.463587Z","shell.execute_reply":"2025-05-23T01:12:38.344446Z"}},"outputs":[{"name":"stdout","text":"Loading en_us dataset...\nDownloaded 3 samples for en_us\nLoading ar_eg dataset...\nDownloaded 3 samples for ar_eg\nLoading fr_fr dataset...\nDownloaded 3 samples for fr_fr\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"metadata_path_test = save_dataset_locally(mydatasets_test,output_dir=\"test_samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:12:38.345920Z","iopub.execute_input":"2025-05-23T01:12:38.346132Z","iopub.status.idle":"2025-05-23T01:12:38.460518Z","shell.execute_reply.started":"2025-05-23T01:12:38.346107Z","shell.execute_reply":"2025-05-23T01:12:38.459873Z"}},"outputs":[{"name":"stdout","text":"Saving en_us samples...\nSaving ar_eg samples...\nSaving fr_fr samples...\nSaved metadata to test_samples/metadata.csv\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"# #caution this delete the dir \n!rm -rf /kaggle/working/new","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:12:38.461427Z","iopub.execute_input":"2025-05-23T01:12:38.461689Z","iopub.status.idle":"2025-05-23T01:12:38.690509Z","shell.execute_reply.started":"2025-05-23T01:12:38.461667Z","shell.execute_reply":"2025-05-23T01:12:38.689751Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# !zip -r /kaggle/working/my_directory.zip /kaggle/working/results/wav2vec2-language-id3/checkpoint-750/optimizer.pt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:12:38.691515Z","iopub.execute_input":"2025-05-23T01:12:38.691780Z","iopub.status.idle":"2025-05-23T01:12:38.696607Z","shell.execute_reply.started":"2025-05-23T01:12:38.691746Z","shell.execute_reply":"2025-05-23T01:12:38.695893Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"from transformers import pipeline\n\n# After training your model:\nclassifier = pipeline(\n    \"audio-classification\",\n    model=loaded_model,\n    feature_extractor=myfeature_extractor\n)\n\n# Then use it like:\nresult = classifier('/kaggle/working/test_samples/fr_fr/fr_fr_2.wav')\nprint(result[0]['label'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:12:38.697371Z","iopub.execute_input":"2025-05-23T01:12:38.697652Z","iopub.status.idle":"2025-05-23T01:12:39.076294Z","shell.execute_reply.started":"2025-05-23T01:12:38.697628Z","shell.execute_reply":"2025-05-23T01:12:39.075707Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"fr_fr\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"import os\nimport glob\nimport pandas as pd\n\ndef predict_directory(model, directory_path):\n    # Create a list to store results\n    results = []\n    \n    # Get all WAV files in the directory\n    audio_files = glob.glob(os.path.join(directory_path, \"*.wav\"))\n    \n    # Process each file\n    for audio_file in audio_files:\n        try:\n            # Get the filename\n            filename = os.path.basename(audio_file)\n            \n            # Make prediction\n            prediction = classifier(audio_file)\n            prediction=(prediction[0]['label'])\n            # Store result\n            results.append({\n                \"file\": filename,\n                \"prediction\": prediction,\n                \"actual\": os.path.basename(directory_path)  # Assuming directory name is the actual label\n            })\n            \n            # Print progress\n            print(f\"Processed {filename}: Predicted {prediction}\")\n            \n        except Exception as e:\n            print(f\"Error processing {audio_file}: {str(e)}\")\n    \n    # Create a DataFrame for better visualization\n    df = pd.DataFrame(results)\n    \n    # Calculate accuracy\n    if len(df) > 0:\n        df[\"correct\"] = df[\"prediction\"] == df[\"actual\"]\n        accuracy = df[\"correct\"].mean() * 100\n        print(f\"\\nAccuracy: {accuracy:.2f}%\")\n    \n    return df\n\n# Test Arabic samples\nar_results = predict_directory(loaded_model, \"/kaggle/working/test_samples/ar_eg\")\nprint(\"\\nArabic Results:\")\ndisplay(ar_results)\n\n# Test English samples\nen_results = predict_directory(loaded_model, \"/kaggle/working/test_samples/en_us\")\nprint(\"\\nEnglish Results:\")\ndisplay(en_results)\n\nfr_results = predict_directory(loaded_model, \"/kaggle/working/test_samples/fr_fr\")\nprint(\"\\nEnglish Results:\")\ndisplay(en_results)\n\n# Combined results\nall_results = pd.concat([ar_results, en_results,fr_results], ignore_index=True)\nprint(\"\\nAll Results:\")\ndisplay(all_results)\n\n# Confusion matrix\nif len(all_results) > 0:\n    from sklearn.metrics import confusion_matrix\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    \n    # Create confusion matrix\n    cm = confusion_matrix(all_results[\"actual\"], all_results[\"prediction\"])\n    \n    # Create DataFrame for better visualization\n    cm_df = pd.DataFrame(\n        cm, \n        index=sorted(all_results[\"actual\"].unique()), \n        columns=sorted(all_results[\"prediction\"].unique())\n        \n    )\n    \n    # Plot\n    plt.figure(figsize=(10, 7))\n    sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n    plt.title(\"Confusion Matrix\")\n    plt.ylabel(\"Actual\")\n    plt.xlabel(\"Predicted\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:12:39.077142Z","iopub.execute_input":"2025-05-23T01:12:39.077439Z","iopub.status.idle":"2025-05-23T01:12:40.742015Z","shell.execute_reply.started":"2025-05-23T01:12:39.077398Z","shell.execute_reply":"2025-05-23T01:12:40.741436Z"}},"outputs":[{"name":"stdout","text":"Processed ar_eg_1.wav: Predicted ar_eg\nProcessed ar_eg_2.wav: Predicted ar_eg\nProcessed ar_eg_0.wav: Predicted ar_eg\n\nAccuracy: 100.00%\n\nArabic Results:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"          file prediction actual  correct\n0  ar_eg_1.wav      ar_eg  ar_eg     True\n1  ar_eg_2.wav      ar_eg  ar_eg     True\n2  ar_eg_0.wav      ar_eg  ar_eg     True","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file</th>\n      <th>prediction</th>\n      <th>actual</th>\n      <th>correct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ar_eg_1.wav</td>\n      <td>ar_eg</td>\n      <td>ar_eg</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ar_eg_2.wav</td>\n      <td>ar_eg</td>\n      <td>ar_eg</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ar_eg_0.wav</td>\n      <td>ar_eg</td>\n      <td>ar_eg</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"Processed en_us_2.wav: Predicted en_us\nProcessed en_us_0.wav: Predicted en_us\nProcessed en_us_1.wav: Predicted en_us\n\nAccuracy: 100.00%\n\nEnglish Results:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"          file prediction actual  correct\n0  en_us_2.wav      en_us  en_us     True\n1  en_us_0.wav      en_us  en_us     True\n2  en_us_1.wav      en_us  en_us     True","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file</th>\n      <th>prediction</th>\n      <th>actual</th>\n      <th>correct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>en_us_2.wav</td>\n      <td>en_us</td>\n      <td>en_us</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>en_us_0.wav</td>\n      <td>en_us</td>\n      <td>en_us</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>en_us_1.wav</td>\n      <td>en_us</td>\n      <td>en_us</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"Processed fr_fr_0.wav: Predicted fr_fr\nProcessed fr_fr_2.wav: Predicted fr_fr\nProcessed fr_fr_1.wav: Predicted fr_fr\n\nAccuracy: 100.00%\n\nEnglish Results:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"          file prediction actual  correct\n0  en_us_2.wav      en_us  en_us     True\n1  en_us_0.wav      en_us  en_us     True\n2  en_us_1.wav      en_us  en_us     True","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file</th>\n      <th>prediction</th>\n      <th>actual</th>\n      <th>correct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>en_us_2.wav</td>\n      <td>en_us</td>\n      <td>en_us</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>en_us_0.wav</td>\n      <td>en_us</td>\n      <td>en_us</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>en_us_1.wav</td>\n      <td>en_us</td>\n      <td>en_us</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\nAll Results:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"          file prediction actual  correct\n0  ar_eg_1.wav      ar_eg  ar_eg     True\n1  ar_eg_2.wav      ar_eg  ar_eg     True\n2  ar_eg_0.wav      ar_eg  ar_eg     True\n3  en_us_2.wav      en_us  en_us     True\n4  en_us_0.wav      en_us  en_us     True\n5  en_us_1.wav      en_us  en_us     True\n6  fr_fr_0.wav      fr_fr  fr_fr     True\n7  fr_fr_2.wav      fr_fr  fr_fr     True\n8  fr_fr_1.wav      fr_fr  fr_fr     True","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file</th>\n      <th>prediction</th>\n      <th>actual</th>\n      <th>correct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ar_eg_1.wav</td>\n      <td>ar_eg</td>\n      <td>ar_eg</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ar_eg_2.wav</td>\n      <td>ar_eg</td>\n      <td>ar_eg</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ar_eg_0.wav</td>\n      <td>ar_eg</td>\n      <td>ar_eg</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>en_us_2.wav</td>\n      <td>en_us</td>\n      <td>en_us</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>en_us_0.wav</td>\n      <td>en_us</td>\n      <td>en_us</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>en_us_1.wav</td>\n      <td>en_us</td>\n      <td>en_us</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>fr_fr_0.wav</td>\n      <td>fr_fr</td>\n      <td>fr_fr</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>fr_fr_2.wav</td>\n      <td>fr_fr</td>\n      <td>fr_fr</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>fr_fr_1.wav</td>\n      <td>fr_fr</td>\n      <td>fr_fr</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x700 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"#done","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T01:12:40.742761Z","iopub.execute_input":"2025-05-23T01:12:40.743012Z","iopub.status.idle":"2025-05-23T01:12:40.747957Z","shell.execute_reply.started":"2025-05-23T01:12:40.742975Z","shell.execute_reply":"2025-05-23T01:12:40.747301Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
